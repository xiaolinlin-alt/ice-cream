# 选做sklearn

要参考sklearn的源码来对比自己的，我们需要先明白实现线性回归的基本原理，线性回归是用来建构出目标和特征之间的线性关系，我们需要最小化预测值与真实值之间的差异，在这里我们有两种方法，一个是最小二乘法，另外一个是梯度下降法（批量）

这边直接看的是sklearn/linear_model/_base.py中的代码

## 区别

* LinearRegression 类继承自 MultiOutputMixin, RegressorMixin, 和 LinearModel，

* 用的方法是最小二乘法，这个数学逻辑基本一致

* 然后功能的具体实现主要在fit函数里

* 会对输入的x和y进行安全性检查，判断x是不是二维的矩阵，是否有空值，然后是将这个一般都会进行的操作封装到类里面

* 代码中对数据进行了预处理，除了我自己写的特征归一化，代码里面对y也做了中心化
* 拟合的时候，将有限制条件的线性优化通过拉格朗日乘子法转化为凸二次规划问题，然后用二次规划的求解方法去解。



## 优化

总之，sklearn里面的源码十分复杂，但是思路清晰，方法复杂但是实现简单，主要优化是在数据的处理和对拟合后的图形误差处理的过程

