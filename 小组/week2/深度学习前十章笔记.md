# 深度学习

### 1.深度学习播放列表概述和机器学习介绍

什么是机器学习：机器学习是使用算法分析数据**，从数据中学习，**然后对新数据做出确定或预测的做法

常规代码：

1. 编写算法。
2. 机器在特定数据集上执行算法。
3. 稍后，机器可以处理以前从未见过的数据执行相同的任务

机器学习：从数据中学习，使用数据和算法进行训练

机器学习的核心思想是：通过大量数据训练模型，使模型能够自动识别模式、做出预测或决策。

### 2.深度学习解释

深度学习是一种可用于实施机器学习的工具或技术。

深度学习是机器学习的一个子领域，它使用受大脑神经网络结构和功能启发的算法。

专注于使用 **多层神经网络** 来模拟和学习数据的复杂模式。

*人工*神经网络 （ANN）：**Artificial Neural Network**，它是一种模仿生物神经系统（如大脑）工作方式的计算模型。ANN 是机器学习和深度学习的核心组成部分，广泛应用于图像识别、自然语言处理、语音识别等领域。

ANN=net=neural net=model

人工神经网络是使用我们所谓的神经元构建的。

ANN 中的神经元被组织成我们所说的层。

ANN *中的*图层（除输入和输出图层之外的所有图层）称为隐藏层。

如果 ANN 具有多个隐藏层，则称该 ANN 为深度 ANN。

### 3.人工神经网络解释

输入层->隐藏层->输出层

~~~~python
from keras.models import Sequential
from keras.layers import Dense,Activation

model=Sequential(layers)
layers=[Dense(units=3,input_shape(2,),activation="relu"),
		Dense(units=2,activation="softmax")
]
~~~~

### 4.神经网络中的层解释

不同类型的层用于处理不同的任务和数据：

- **全连接层**：学习全局特征。
- **卷积层**：提取局部特征。
- **池化层**：降低数据维度。
- **循环层**：处理序列数据。
- **嵌入层**：将离散数据映射为连续向量。
- **归一化层**：加速训练并提高稳定性。
- **Dropout 层**：防止过拟合。
- **扁平化层**：将多维数据展平。
- **输入层**：定义输入形状。
- **输出层**：生成最终预测结果。
- **自注意力层**：捕捉全局依赖关系

为什么会有不同类型的层：不同的层对其输入执行不同的转换，并且某些层比其他层更适合某些任务。

**图层权重：**

权重（Weight）

- **定义**：权重是神经元之间的连接强度，决定了输入信号对输出的影响。
- **训练目标**：通过调整权重，使模型的预测结果尽可能接近真实值。

两个节点之间的每个连接都有一个关联的权重，它只是一个数字。

每个权重表示两个节点之间的连接强度。当网络在输入层的给定节点收到输入时，此输入将通过连接传递到下一个节点，并且输入将乘以权重 分配给该连接。

 **前向传播（Forward Propagation）**

- **定义**：输入数据通过神经网络的每一层，最终生成输出。
- **过程**：
  1. 输入数据传递给输入层。
  2. 每一层的神经元对输入进行加权求和，并通过激活函数生成输出。
  3. 输出层生成最终的预测结果。

~~~~python
from keras.models import Sequential
from keras.layers import Dense, Activation

layers = [
    Dense(units=6, input_shape=(8,), activation='relu'),
    Dense(units=6, activation='relu'),
    Dense(units=4, activation='softmax')
]

model = Sequential(layers)
~~~~



### 5.神经网络中的激活函数解释

**激活函数Activation Function**：在人工神经网络中，激活函数是将节点的输入映射到其相应输出的函数，

**非线性**喽

激活函数的作用公式引入非线性，使神经网络能够学习复杂的模式

常见的有：

- **Sigmoid**：将输入映射到 (0, 1) 之间。
  $$
  f(x) = \frac{1}{1 + e^{-x}}
  $$
  

  输出范围0-1

  常用于二分类问题

  输出为概率

  梯度容易消失

- **ReLU**：如果输入大于 0，输出输入值；否则输出 0。

  修正线性单元
  $$
  f(x)=max(0,x)
  $$
  输出范围0到正无穷

  目前最常用

  ~~~~python
  def RELU(x):
  	if(x<=0)
  	{
  		reruen 0;
  	}
  	else
  	{
  		return x;
  	}
  ~~~~

  

- **Tanh**：将输入映射到 (-1, 1) 之间。

  双曲正切函数
  $$
  f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
  $$
  输出以0为中心，梯度更大

  

我们获取层中每个节点的每个传入连接的加权和，并将该加权和传递给激活函数

节点输出 = 激活（输入的加权和）

**为什么使用激活函数？**

线性意味着在forward的时候只对数据值进行线性转换

具有非线性激活函数使我们的神经网络能够计算任意复杂的函数。

proof that RELU is Non-Linear
$$
f(x)=relu(x)
$$
if a<0
$$
f(-1a)=max(0,-1a)>0
$$

$$
(-1)f(a)=(-1)max(0,-1a)=0
$$

$$
f(-1a)!=(-1)f(a)
$$



### 6.训练神经网络

什么是训练

尝试优化模型中的权重。我们的任务是找到最准确地将输入数据映射到正确输出类的权重。这 映射是网络必须*学习*的内容。

优化算法：

随机梯度下降SGD

目标：最小化我们称为*损失函数*的某些给定函数。因此，SGD 以使此损失函数尽可能接近其最小值的方式更新模型的权重。

损失函数：一种常见的损失函数是*均方误差* （MSE）

预测与真实之间的差距

### 7.神经网络学习如何解释

***模型学习*究竟意味着什么呢？**

在初始化模型时，网络权重将设置为任意值。我们还看到，在网络的末端，模型将为给定的输入提供输出

获得输出后，可以通过查看模型预测的内容与真实标签来计算该特定输出的损失（或误差）。损失计算取决于所选的损失函数

**损失函数的梯度**：

一旦我们有了损失函数的梯度值，我们就可以使用这个值来更新模型的权重。梯度告诉我们哪个方向会将损失推向最小，我们的任务是朝着降低 loss 并逐渐接近此最小值。

**学习率**

学习率是一个较小的数字，通常介于 0.01 和 0.0001 之间，但实际值可能会有所不同。

**学习率告诉我们应该朝着最小方向迈出多大的一步。**

updata：
$$
new weight=old weight-(learning rate*grad)
$$

### 8.神经网络中的损失解释

损失函数是 SGD 试图通过迭代更新网络中的权重来最小化的函数。

对单个样本：预测与标签之间的差异（误差）。然后我们调整此误差的平方

如果我们一次将整个训练集传递给模型，那么我们刚刚计算损失的过程将在训练期间的每个 epoch 结束时发生。

### 9.神经网络中的学习率解释

我们为达到最小损失而采取的这些步骤的大小将取决于学习率。从概念上讲，我们可以将模型的学习率视为*步长*。

### 10.训练、测试和验证集解释

数据分解为三个不同的数据集

- 训练集：用于训练模型的数据集
- 验证集：用于在训练期间验证我们的模型。。权重 不会根据根据我们的验证数据计算的损失在模型中进行更新。此数据不包含模型已经熟悉的训练样本。

​       **验证集允许我们查看模型在训练期间的泛化程度。**

- 测试集：用于在模型训练后测试模型



### 11.使用神经网络进行预测

预测基于模型在训练期间学到的内容。

### 12神经网络中的过拟合解释

过拟合是指模型在训练数据上表现很好，但在新数据上表现较差的现象。通常是因为模型过于复杂，过度学习了训练数据中的噪声和细节，导致泛化能力下降。

**减小过拟合的方法**

1. **增加数据量**：
   - 获取更多数据有助于模型更好地泛化。
2. **数据增强**：
   - 对现有数据进行变换（如旋转、缩放等），生成更多训练样本。
3. **简化模型**：
   - 减少模型复杂度，如降低神经网络层数或神经元数量。
4. **正则化**：
   - 在损失函数中加入正则项（如L1、L2正则化），限制参数大小。
5. **Dropout**：
   - 在训练时随机丢弃部分神经元，防止过度依赖某些特征。
6. **早停**：
   - 监控验证集性能，在性能不再提升时提前停止训练。
7. **交叉验证**：
   - 使用交叉验证评估模型，确保其在不同数据子集上表现稳定。
8. **集成学习**：
   - 结合多个模型的预测，如Bagging、Boosting等，提升泛化能力。
9. **减少特征**：
   - 通过特征选择去除冗余或不相关特征，降低模型复杂度。

可通过增加数据、简化模型、正则化、Dropout、早停等方法有效缓解。

### 13.神经网络中的欠拟合解释

当模型无法对训练它所依据的数据进行分类时，它被称为欠拟合

通常是因为模型过于简单，无法捕捉数据中的复杂模式。

**减小欠拟合的方法**

1. **增加模型复杂度**：
   - 使用更复杂的模型，如增加神经网络层数或神经元数量。
2. **增加特征**：
   - 引入更多相关特征，帮助模型更好地理解数据。
3. **减少正则化**：
   - 降低正则化强度（如L1、L2正则化），避免过度限制模型。
4. **延长训练时间**：
   - 增加训练轮数，确保模型充分学习数据。
5. **使用更复杂的算法**：
   - 选择更强大的算法，如从线性模型切换到非线性模型（如决策树、神经网络）。
6. **特征工程**：
   - 通过特征组合、多项式特征等方法，生成更有意义的特征。
7. **调整超参数**：
   - 优化学习率、批量大小等超参数，提升模型性能。
8. **集成学习**：
   - 使用集成方法（如Bagging、Boosting）结合多个模型，提升预测能力。



### 14.监督式学习解释

标签用于监督或指导学习过程。

### 15.无监督学习解释

无监督学习发生在未标记的数据上。

*聚类算法*。

### 16.半监督学习解释

*已标记*和未标记数据的组合。

伪标记使我们能够在更大的数据集上进行训练。

### 17.数据增强说明

常见的数据增强技术。

- 水平翻转
- 垂直翻转
- 旋转
- 放大
- 缩小
- 种植
- 颜色变化

对训练集中的数据进行合理的修改来创建新的增强数据。

**当我们根据对现有数据的修改创建新数据时，就会*发生数据增强***

### 18.One-hot Encoding

 Keras 中图像的标签实际上是 *one-hot 编码向量*

如果我们的模型是一个图像分类器，我们可能会将标记的动物图像作为输入传递。当我们这样做时，模型通常不会将这些标签解释为单词，例如 *dog* 或 *cat*。此外，我们的模型给出的预测输出通常也不是 *dog* 或 *cat* 之类的词。相反，大多数时候我们的标签会被编码，因此它们可以采用整数或整数向量的形式。

**One-hot编码**是一种将分类变量转换为数值形式的方法，常用于机器学习和数据处理。它将每个类别转换为一个二进制向量，其中只有一个元素为1，其余为0。

**示例**

假设有一个颜色类别：红、绿、蓝。

- 红: [1, 0, 0]
- 绿: [0, 1, 0]
- 蓝: [0, 0, 1]

**优点**

1. **简单直观**：易于理解和实现。
2. **兼容算法**：适合大多数机器学习算法，尤其是不能直接处理类别数据的模型。

**缺点**

1. **维度膨胀**：类别较多时，会导致特征维度大幅增加。
2. **稀疏性**：生成的向量通常非常稀疏，可能影响计算效率。

**应用场景**

- **分类变量**：用于处理无序的分类数据。
- **自然语言处理**：将单词或字符转换为向量表示。

### 19.卷积神经网络 （CNN） 解释

什么是CNN

卷积神经网络，也称为 *CNN* 或 *ConvNet*，是一种人工神经网络，迄今为止最广泛地用于分析计算机视觉任务的图像

它具有某种类型的专业化，能够识别或检测模式。这种模式检测使 CNN 对图像分析如此有用。

**CNN 具有称为*卷积层的*层。**

那什么是卷积层？

就像任何其他层一样，卷积层接收输入，以某种方式转换输入，然后将转换后的输入输出到下一层。卷积层的输入称为 input channels，输出称为output channels

对于卷积层，发生的转换称为卷积运算

过滤器：对于每个卷积层，我们需要指定该层应具有的过滤器数量。这些过滤器实际上是检测模式的过滤器。

多个边缘、形状、纹理、对象等。这就是我们所说的*模式*。

- 边缘-->边缘过滤器
- 形状
- 纹理
- 曲线
- 对象
- 颜色

过滤器（pattern detectors）

过滤器的数量决定了输出通道的数量。

过滤器可以被认为是一个相对较小的矩阵（张量），为此，我们决定这个矩阵的行数和列数，并且这个矩阵中的值用随机数初始化。

![image-20250324154836367](https://yuyingcun.oss-cn-guangzhou.aliyuncs.com/typora/202503241548605.png)

模式检测器随着网络的学习而出现

### 20.可视化来自 CNN 的卷积滤波器

### 21.卷积神经网络中的零填充解释

### 22.卷积神经网络中的最大池化解释

### 23.反向传播解释 |第 1 部分 - 直觉

### 24.反向传播解释 |第 2 部分 - 数学符号

### 25.反向传播解释 |第 3 部分 - 数学观察]

### 26.反向传播解释 |第 4 部分 - 计算梯度

### 27.反向传播解释 |第 5 部分 - 什么将 “back” 放入反向传播中？

### 28.消失和爆炸梯度解释 |反向传播导致的问题

### 29.权重初始化说明 |减少梯度消失问题的方法

### 30.人工神经网络中的偏差详解 |偏见如何影响培训

### 31.人工神经网络中的可学习参数解释

### 32.卷积神经网络 （CNN） 中的可学习参数解释

### 33.神经网络中的正则化解释

### 34.神经网络中的批量大小解释

### 35.微调神经网络解释

### 36.批量标准化（“批量规范”）解释